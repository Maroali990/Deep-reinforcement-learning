{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "E1jsBF16L8Zu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yRMb-GF7-L_V"
   },
   "outputs": [],
   "source": [
    "# Create Gridworld object\n",
    "\n",
    "class Gridworld:\n",
    "    \n",
    "    def __init__(self,dim):\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.gid_world = []\n",
    "        self.reward_world = []\n",
    "        \n",
    "        self.gid_world_original = []\n",
    "        self.reward_world_original = []\n",
    "        \n",
    "        self.free_fields = []\n",
    "        self.goal = []\n",
    "        self.tele_idx = []\n",
    "        \n",
    "        self.finished = False\n",
    "    \n",
    "    def build(self):\n",
    "        \n",
    "        ######### Build GW #########\n",
    "\n",
    "        for x in range(self.dim+1):\n",
    "            self.grid_world = [\"O\"] * (x * x)\n",
    "\n",
    "        # Create obstacles\n",
    "\n",
    "        self.grid_world = np.array(self.grid_world)\n",
    "\n",
    "        num_obs = int((self.dim*self.dim)/8) # specify number of obstacles\n",
    "\n",
    "        obstacle_indices = np.random.choice(np.arange(1,self.grid_world.size), replace=False, size=num_obs) # start at one to not place obstacles at player pos\n",
    "\n",
    "        self.grid_world[obstacle_indices] = \"X\"\n",
    "        \n",
    "        ######### Build RW and Teleport #########\n",
    "\n",
    "        # Assign rewards to states\n",
    "\n",
    "        self.reward_world =copy.deepcopy(self.grid_world.flatten()) # create seperate array for reward\n",
    "\n",
    "        self.free_fields = [x for x in np.arange(self.reward_world.size) if x not in obstacle_indices if x != 0] # generate list of free fields\n",
    "\n",
    "        np.random.shuffle(self.free_fields) # shuffle free fields\n",
    "\n",
    "        self.goal = self.free_fields[-1] # choose index for postive reward (terminal state)\n",
    "        \n",
    "        self.tele_idx = [self.free_fields[-2],self.free_fields[-3]]# choose 2 indices for teleports\n",
    "        \n",
    "        rew_neg_amount = 5 # specify number of negative rewards\n",
    "\n",
    "        rew_neg_indices = np.random.choice(self.free_fields[1:-3], replace=False, size=rew_neg_amount) # randomly choose incides for negative rewards\n",
    "\n",
    "        self.reward_world[rew_neg_indices] = \"-\" # place negative rewards\n",
    "\n",
    "        self.reward_world[self.goal] = \"+\" # place positive reward (terminal state)\n",
    "        \n",
    "        self.reward_world[self.tele_idx] = \"ยง\" # place teleports\n",
    "        \n",
    "        \n",
    "        # Set starting point #\n",
    "\n",
    "        self.grid_world[0] = \"P\" # top left\n",
    "        \n",
    "\n",
    "        # Save Backup\n",
    "\n",
    "        self.reward_world_original = copy.deepcopy(self.reward_world) \n",
    "        \n",
    "        self.grid_world_original = copy.deepcopy(self.grid_world)\n",
    "        \n",
    "        \n",
    "        return self.grid_world, self.reward_world\n",
    "    \n",
    "    def reset(self):\n",
    "        \n",
    "        self.reward_world = copy.deepcopy(self.reward_world_original)\n",
    "        self.grid_world = copy.deepcopy(self.grid_world_original)\n",
    "\n",
    "        return self.grid_world, self.reward_world\n",
    "\n",
    "    def move(self, action):\n",
    "        \n",
    "        idx = np.where(self.grid_world == \"P\")[0]\n",
    "        idx = idx[0]\n",
    "        \n",
    "        self.reward = -0.3 # immediate penalty of -0.3 for all moves that dont land on a reward field\n",
    "        \n",
    "        if action == \"left\":\n",
    "            \n",
    "            # if path is Oob\n",
    "            \n",
    "            if idx == 0 or (idx%self.dim) == 0:\n",
    "        \n",
    "                return self.grid_world, self.finished, self.reward\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                if self.grid_world[idx-1]  == \"X\":\n",
    "                    \n",
    "                    return self.grid_world, self.finished, self.reward\n",
    "                \n",
    "                # check for teleports and teleport with 60% probability\n",
    "                \n",
    "                elif idx-1 == self.tele_idx[0] and random.randint(0,100) > 40:\n",
    "                    \n",
    "                    # teleport player\n",
    "                    \n",
    "                    self.grid_world[idx] = \"O\"\n",
    "                    \n",
    "                    self.grid_world[self.tele_idx[1]] = \"P\"\n",
    "                    \n",
    "                    return self.grid_world, self.finished, self.reward\n",
    "                    \n",
    "                elif idx-1 == self.tele_idx[1] and random.randint(0,100) > 40:\n",
    "                    \n",
    "                    # teleport player\n",
    "                    \n",
    "                    self.grid_world[idx] = \"O\"\n",
    "                    \n",
    "                    self.grid_world[self.tele_idx[0]] = \"P\"\n",
    "                    \n",
    "                    return self.grid_world, self.finished, self.reward\n",
    "                   \n",
    "                # if path isnt blocked\n",
    "                \n",
    "                else:\n",
    "                    \n",
    "                    # check for rewards\n",
    "                    \n",
    "                    if self.reward_world[idx-1] == \"+\":\n",
    "                        \n",
    "                        self.reward = +100\n",
    "                        self.finished = True\n",
    "                        \n",
    "                    elif self.reward_world[idx-1] == \"-\":\n",
    "                        \n",
    "                        self.reward = -10\n",
    "                        \n",
    "                    # move player\n",
    "                    \n",
    "                    self.grid_world[idx] = \"O\"\n",
    "                    \n",
    "                    self.grid_world[idx-1] = \"P\"\n",
    "                \n",
    "                return self.grid_world, self.finished, self.reward\n",
    "            \n",
    "        elif action == \"right\":\n",
    "            \n",
    "            # if path is Oob\n",
    "            \n",
    "            if idx == (self.dim-1) or idx == (len(self.grid_world)-1):\n",
    "                \n",
    "                return self.grid_world, self.finished, self.reward\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                if self.grid_world[idx+1] == \"X\":\n",
    "                    \n",
    "                    return self.grid_world, self.finished, self.reward\n",
    "                \n",
    "                # check for teleports and teleport with 60% probability\n",
    "                \n",
    "                elif idx+1 == self.tele_idx[0] and random.randint(0,100) > 40:\n",
    "                    \n",
    "                    # teleport player\n",
    "                    \n",
    "                    self.grid_world[idx] = \"O\"\n",
    "                    \n",
    "                    self.grid_world[self.tele_idx[1]] = \"P\"\n",
    "                    \n",
    "                    return self.grid_world, self.finished, self.reward\n",
    "                    \n",
    "                elif idx+1 == self.tele_idx[1] and random.randint(0,100) > 40:\n",
    "                    \n",
    "                    # teleport player\n",
    "                    \n",
    "                    self.grid_world[idx] = \"O\"\n",
    "                    \n",
    "                    self.grid_world[self.tele_idx[0]] = \"P\"\n",
    "                    \n",
    "                    return self.grid_world, self.finished, self.reward\n",
    "                \n",
    "                # if path isnt blocked\n",
    "                \n",
    "                else:\n",
    "                    \n",
    "                    # check for rewards\n",
    "                    \n",
    "                    if self.reward_world[idx+1] == \"+\":\n",
    "                        \n",
    "                        self.reward = +1\n",
    "                        self.finished = True\n",
    "                        \n",
    "                    elif self.reward_world[idx+1] == \"-\":\n",
    "                        \n",
    "                        self.reward = -1\n",
    "                        \n",
    "                    # move player\n",
    "                    self.grid_world[idx] = \"O\"\n",
    "                    \n",
    "                    self.grid_world[idx+1] = \"P\"\n",
    "                    \n",
    "                    return self.grid_world, self.finished, self.reward\n",
    "            \n",
    "        elif action == \"up\":\n",
    "            \n",
    "             # if path is Oob\n",
    "            \n",
    "            if idx in range(0,(self.dim-1)):\n",
    "                \n",
    "                return self.grid_world, self.finished, self.reward\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                if self.grid_world[idx-self.dim] == \"X\":\n",
    "                    \n",
    "                    return self.grid_world, self.finished, self.reward\n",
    "                \n",
    "                # check for teleports and teleport with 60% probability\n",
    "                \n",
    "                elif idx-self.dim == self.tele_idx[0] and random.randint(0,100) > 40:\n",
    "                    \n",
    "                    # teleport player\n",
    "                    \n",
    "                    self.grid_world[idx] = \"O\"\n",
    "                    \n",
    "                    self.grid_world[self.tele_idx[1]] = \"P\"\n",
    "                    \n",
    "                    return self.grid_world, self.finished, self.reward\n",
    "                    \n",
    "                elif idx-self.dim == self.tele_idx[1] and random.randint(0,100) > 40:\n",
    "                    \n",
    "                    # teleport player\n",
    "                    \n",
    "                    self.grid_world[idx] = \"O\"\n",
    "                    \n",
    "                    self.grid_world[self.tele_idx[0]] = \"P\"\n",
    "                    \n",
    "                    return self.grid_world, self.finished, self.reward\n",
    "                \n",
    "                # if path isnt blocked\n",
    "                \n",
    "                else:\n",
    "                    \n",
    "                    # check for rewards\n",
    "                    \n",
    "                    if self.reward_world[idx-self.dim] == \"+\":\n",
    "                        \n",
    "                        self.reward = +1\n",
    "                        self.finished = True\n",
    "                        \n",
    "                    elif self.reward_world[idx-self.dim] == \"-\":\n",
    "                        \n",
    "                        self.reward = -1\n",
    "                        \n",
    "                    # move player\n",
    "                    \n",
    "                    self.grid_world[idx] = \"O\"\n",
    "                    \n",
    "                    self.grid_world[idx-self.dim] = \"P\"\n",
    "                    \n",
    "                    return self.grid_world, self.finished, self.reward\n",
    "            \n",
    "        elif action == \"down\":\n",
    "            \n",
    "             # if path is Oob\n",
    "            \n",
    "            if idx in range(len(self.grid_world)-self.dim,len(self.grid_world)):\n",
    "                \n",
    "                return self.grid_world, self.finished, self.reward\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                if self.grid_world[idx+self.dim] == \"X\":\n",
    "                    \n",
    "                    return self.grid_world, self.finished, self.reward\n",
    "                \n",
    "                # check for teleports and teleport with 60% probability\n",
    "                \n",
    "                elif idx+self.dim == self.tele_idx[0] and random.randint(0,100) > 40:\n",
    "                    \n",
    "                    # teleport player\n",
    "                    \n",
    "                    self.grid_world[idx] = \"O\"\n",
    "                    \n",
    "                    self.grid_world[self.tele_idx[1]] = \"P\"\n",
    "                    \n",
    "                    return self.grid_world, self.finished, self.reward\n",
    "                    \n",
    "                elif idx+self.dim == self.tele_idx[1] and random.randint(0,100) > 40:\n",
    "                    \n",
    "                    # teleport player\n",
    "                    \n",
    "                    self.grid_world[idx] = \"O\"\n",
    "                    \n",
    "                    self.grid_world[self.tele_idx[0]] = \"P\"\n",
    "                    \n",
    "                    return self.grid_world, self.finished, self.reward\n",
    "                \n",
    "                # if path isnt blocked\n",
    "                \n",
    "                else:\n",
    "                    \n",
    "                    # check for rewards\n",
    "                    \n",
    "                    if self.reward_world[idx+self.dim] == \"+\":\n",
    "                        \n",
    "                        self.reward = +1\n",
    "                        self.finished = True\n",
    "                        \n",
    "                    elif self.reward_world[idx+self.dim] == \"-\":\n",
    "                        \n",
    "                        self.reward = -1\n",
    "                        \n",
    "                    # move player\n",
    "                    \n",
    "                    self.grid_world[idx] = \"O\"\n",
    "                    \n",
    "                    self.grid_world[idx+self.dim] = \"P\"\n",
    "                    \n",
    "                    return self.grid_world, self.finished, self.reward\n",
    "            \n",
    "        else:\n",
    "            print(\"Please choose an action [left,right,up,down]!\")        \n",
    "    \n",
    "    def visualize(self):\n",
    "        \n",
    "        # Show GW\n",
    "        \n",
    "        print(\"Gridworld:\\n\")\n",
    "        print(self.grid_world.reshape(((self.dim, self.dim))))\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # Show RW\n",
    "\n",
    "        print(\"Rewardworld:\\n\")\n",
    "        print(self.reward_world.reshape(((self.dim, self.dim))))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jhdb6GBm-ZR0",
    "outputId": "bff39063-2885-44a9-f5d6-7502ecdbf6f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your desired grid dimension (dim X dim):\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# Specify dimensions\n",
    "\n",
    "while True:    \n",
    "    try:\n",
    "        dim = int(input(\"Please provide your desired grid dimension (dim X dim):\\n\"))\n",
    "        \n",
    "        if dim >= 5:\n",
    "            break\n",
    "            \n",
    "        print(\"Dimension needs to be larger than 4!\\n\")\n",
    "    \n",
    "    except:\n",
    "        print(\"Please provide an integer value!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ic3vK2kG-fvk",
    "outputId": "c2ec3595-2d05-4a28-c070-6a83e3e70557"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gridworld:\n",
      "\n",
      "[['P' 'O' 'O' 'O' 'O' 'X' 'O' 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' 'X' 'O' 'O' 'O' 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'X']\n",
      " ['X' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' 'O' 'O' 'O' 'X' 'X' 'O' 'O']\n",
      " ['O' 'X' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' 'O' 'O' 'O' 'X' 'O' 'X' 'O']\n",
      " ['O' 'O' 'X' 'O' 'O' 'O' 'O' 'O' 'O' 'O']\n",
      " ['X' 'O' 'O' 'O' 'O' 'O' 'X' 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O']]\n",
      "\n",
      "\n",
      "Rewardworld:\n",
      "\n",
      "[['O' 'O' 'O' 'O' 'O' 'X' '-' 'O' 'O' 'O']\n",
      " ['O' 'O' '-' 'X' 'O' 'O' 'O' 'O' 'O' 'O']\n",
      " ['ยง' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'X']\n",
      " ['X' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' 'O' 'O' 'O' 'X' 'X' 'O' 'O']\n",
      " ['O' 'X' 'O' '-' 'O' 'O' 'O' 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' 'O' 'O' 'O' 'X' 'O' 'X' 'O']\n",
      " ['O' 'O' 'X' 'O' '-' '-' 'O' 'O' 'O' 'O']\n",
      " ['X' '+' 'O' 'O' 'O' 'O' 'X' 'O' 'ยง' 'O']\n",
      " ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O']]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create GW object\n",
    "\n",
    "gw = Gridworld(dim)\n",
    "\n",
    "gw.build()\n",
    "\n",
    "# Show GW\n",
    "#the state in the reward world with + is the terminal state\n",
    "\n",
    "gw.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gw.move(\"right\")\n",
    "# gw.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gw.reset()\n",
    "# gw.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57AJ24I9AdwW"
   },
   "source": [
    "n-step Sarsa, this code is taken from https://www.geeksforgeeks.org/sarsa-reinforcement-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4Bj7W5ocAdXK"
   },
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "# set initial parameters\n",
    "epsilon = 0.9\n",
    "total_episodes = 2000000\n",
    "max_steps = 150\n",
    "alpha = 0.8\n",
    "gamma = 0.9\n",
    "\n",
    "# define actionspace\n",
    "action_space=[0,1,2,3]\n",
    "dict_moves={0:\"left\",1:\"right\",2:\"up\",3:\"down\"}\n",
    "\n",
    "# list to save the SAR\n",
    "n_list=[]\n",
    " \n",
    "# initialize Q value table \n",
    "Q = np.zeros((dim*dim,len(action_space))) # dim: number of states x actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QLreRulLC0a2"
   },
   "outputs": [],
   "source": [
    "def choose_action(state):\n",
    "    \n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        \n",
    "        # sample random action\n",
    "        action = random.sample(action_space,1)[0] \n",
    "        \n",
    "        return action\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # get best action (maximize Q value)\n",
    "        action = np.argmax(Q[state, :]) # requires index of the player (state)\n",
    "        \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FXv9Owq4C9hJ"
   },
   "outputs": [],
   "source": [
    "def update(state1,state2,reward,action1,action2):\n",
    "    \n",
    "#     print(\"current state (s): \",state1, \"\\n\", \"next state (s'): \",\n",
    "#           state2,\"\\n\",\"reward (r): \",reward,\"\\n\", \"current action (a): \",\n",
    "#           dict_moves[action1],\"\\n\", \"next action (a'): \",dict_moves[action2],\"\\n\")\n",
    "    \n",
    "    idx1 = np.where(state1 == \"P\")[0][0]\n",
    "    #print(\"index 1: \",idx1)\n",
    "    \n",
    "    idx2 = np.where(state2 ==\"P\")[0][0]\n",
    "    #print(\"index 2: \",idx2)\n",
    "    \n",
    "    # get estimates\n",
    "    Q_old = Q[idx1, action1]   \n",
    "    Q_new = Q[idx2, action2]\n",
    "    Q[idx1, action1] = Q_old + alpha * (reward + (gamma *  Q_new) - Q_old)\n",
    "    \n",
    "#     print(\"Q_old: \", Q_old)\n",
    "#     print(\"Q_new: \", Q_new)\n",
    "#     print((\"Reward: \", reward))\n",
    "#     print(\"Q_calc: \", (Q_old + alpha * (reward + (gamma *  Q_new) - Q_old)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "U9gWLz-5B-KN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100000\n",
      "Epoch: 200000\n",
      "Epoch: 300000\n",
      "Epoch: 400000\n",
      "Epoch: 500000\n",
      "Epoch: 600000\n",
      "Epoch: 700000\n",
      "Epoch: 800000\n",
      "Epoch: 900000\n",
      "Epoch: 1000000\n",
      "Epoch: 1100000\n",
      "Epoch: 1200000\n",
      "Epoch: 1300000\n",
      "Epoch: 1400000\n",
      "Epoch: 1500000\n",
      "Epoch: 1600000\n",
      "Epoch: 1700000\n",
      "Epoch: 1800000\n",
      "Epoch: 1900000\n",
      "Learning finished!\n"
     ]
    }
   ],
   "source": [
    "# initializing the reward\n",
    "reward = 0\n",
    "\n",
    "# epoch_guide\n",
    "\n",
    "ep_ = 0\n",
    "\n",
    "# Starting the SARSA learning\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    \n",
    "    # every 100000 epochs\n",
    "    if (ep_ % 100000) == 0 and ep_ != 0:\n",
    "        print(\"Epoch: \" + str(ep_))\n",
    "        \n",
    "#         # check exploration status\n",
    "#         unique, counts = np.unique(Q, return_counts=True)\n",
    "#         tmp = dict(zip(unique, counts))\n",
    "#         if tmp[0] < ((dim*dim)/4): # if a less then a quarter of Q values is unexplored end the algorithm\n",
    "#             break\n",
    "#         else:\n",
    "#             print(\"Unexplored: \",(tmp[0]/Q.size)*100, \"%\")\n",
    "    \n",
    "    t = 0 # reset temperatur during episodes\n",
    "    state1, _ = gw.reset() # reset gridworld and get initial state    \n",
    "    idx_state1 = np.where(state1 == \"P\")[0][0] # state needs to be an index for the condition work \n",
    "    action1 = choose_action(idx_state1) # get current action\n",
    "\n",
    "    while t < max_steps: \n",
    "         \n",
    "        # getting the next state\n",
    "        state2, done, reward = gw.move(dict_moves[action1])\n",
    "        \n",
    "        # get index of player in next state\n",
    "        idx_state2 = np.where(state2 == \"P\")[0][0]\n",
    "        \n",
    "        # choose the next action\n",
    "        action2 = choose_action(idx_state2) # state needs to be an index for the condition work\n",
    "         \n",
    "        # calculate the Q-value\n",
    "        update(state1, state2, reward, action1, action2)\n",
    "         \n",
    "        # update state and action\n",
    "        state1 = state2\n",
    "        action1 = action2\n",
    "         \n",
    "        # update temperature\n",
    "        t += 1\n",
    "        reward += reward\n",
    "         \n",
    "        #If at the end of learning process\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    ep_ += 1\n",
    "            \n",
    "print(\"Learning finished!\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "yNr6j-aLADOQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.58463444  0.         -0.38806397  0.        ]\n",
      " [-0.76879872 -0.5563173  -1.01004502  0.        ]\n",
      " [-0.8143852  -0.59684928 -0.75232295  0.        ]\n",
      " [-0.24       -0.91230847 -0.52992    -0.49904507]\n",
      " [ 0.         -0.2976      0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-8.08064     0.         -0.98752     0.        ]\n",
      " [ 0.         -0.4608      0.          0.        ]\n",
      " [ 0.         -0.24       -0.288       0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.3427584   0.          0.         -0.35962914]\n",
      " [-0.4608     -0.84191846 -0.6957312  -0.619776  ]\n",
      " [ 0.         -1.557632   -0.96       -1.15968   ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.288       0.          0.         -0.32256   ]\n",
      " [ 0.         -0.4608     -0.24        0.        ]\n",
      " [ 0.         -0.288       0.          0.        ]\n",
      " [-0.288      -0.24        0.          0.        ]\n",
      " [-0.24       -0.24       -0.4608     -0.4128    ]\n",
      " [ 0.         -0.50496     0.          0.        ]\n",
      " [-0.288       0.          0.         -0.2976    ]\n",
      " [-0.4608     -0.4608     -0.24       -0.672768  ]\n",
      " [-0.24       -0.5317632  -0.4128     -0.537216  ]\n",
      " [-0.63426798 -0.24       -0.5707706   0.        ]\n",
      " [-0.6861312  -0.4608      0.         -0.24      ]\n",
      " [-0.454272   -0.2976     -0.4608      0.        ]\n",
      " [-0.4608     -0.24        0.          0.        ]\n",
      " [ 0.         -0.4608      0.         -0.288     ]\n",
      " [ 0.         -0.24       -0.50496     0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.299904    0.         -0.288       0.        ]\n",
      " [ 0.         -0.514176   -0.24       -0.288     ]\n",
      " [-0.4128     -0.24       -0.4608     -0.4128    ]\n",
      " [ 0.         -0.7045632  -0.24        0.        ]\n",
      " [ 0.         -0.288       0.         -0.49536   ]\n",
      " [ 0.         -0.4608      0.          0.        ]\n",
      " [ 0.         -0.4608      0.         -0.7045632 ]\n",
      " [ 0.         -0.6542592  -0.91048317 -0.75906662]\n",
      " [ 0.         -0.288       0.          0.        ]\n",
      " [-0.38838804 -0.24       -0.55427604  0.        ]\n",
      " [ 0.         -0.33216     0.         -0.24      ]\n",
      " [-0.585216   -0.24        0.         -0.4128    ]\n",
      " [-0.4608     -0.571776   -0.4608      0.        ]\n",
      " [-0.24       -0.44736     0.         -0.288     ]\n",
      " [ 0.         -0.24       -0.288       0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.3358464   0.         -0.24       -0.33216   ]\n",
      " [ 0.         -0.663936   -0.24       -0.24      ]\n",
      " [-0.44736    -0.24       -0.95060152 -0.672768  ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.34587857  0.          0.         -0.288     ]\n",
      " [ 0.         -0.9984      0.          0.        ]\n",
      " [ 0.          0.          0.         -0.24      ]\n",
      " [-0.34404864 -0.24        0.         -0.4128    ]\n",
      " [-0.24       -0.4608      0.         -0.24      ]\n",
      " [-0.24        0.         -0.24        0.        ]\n",
      " [ 0.         -0.24        0.         -0.288     ]\n",
      " [ 0.          0.         -0.24       -0.24      ]\n",
      " [-0.30498068 -0.49828592 -0.62189374 -0.35873045]\n",
      " [ 0.         -0.304512   -0.38091264  0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.24        0.          0.          0.        ]\n",
      " [ 0.         -0.4128      0.         -0.24      ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.         -0.24        0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.4128      0.         -0.24        0.        ]\n",
      " [-0.43083033  0.          0.         -0.35460305]\n",
      " [ 0.         -0.52191744  0.         -0.288     ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-8.          0.          0.          0.        ]\n",
      " [ 0.         -1.376      -1.79072    -0.8       ]\n",
      " [ 0.         -0.24       -0.4128      0.        ]\n",
      " [ 0.         -0.24        0.          0.        ]\n",
      " [-0.24        0.         -0.288       0.        ]\n",
      " [ 0.         -0.24       -0.4608      0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.8       ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.         -0.24      ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.32256    -0.288       0.          0.        ]\n",
      " [ 0.         -0.24       -0.50496    -0.6035712 ]\n",
      " [-0.24       -0.24        0.          0.        ]\n",
      " [ 0.         -0.24        0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.288       0.          0.          0.        ]\n",
      " [-0.45692621 -0.24        0.         -0.47305728]\n",
      " [ 0.         -0.2976      0.         -0.24      ]]\n"
     ]
    }
   ],
   "source": [
    "# Print final Q value table\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gridworld:\n",
      "\n",
      "[['P' 'O' 'O' 'O' 'O' 'X' 'O' 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' 'X' 'O' 'O' 'O' 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'X']\n",
      " ['X' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' 'O' 'O' 'O' 'X' 'X' 'O' 'O']\n",
      " ['O' 'X' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' 'O' 'O' 'O' 'X' 'O' 'X' 'O']\n",
      " ['O' 'O' 'X' 'O' 'O' 'O' 'O' 'O' 'O' 'O']\n",
      " ['X' 'O' 'O' 'O' 'O' 'O' 'X' 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O']]\n",
      "\n",
      "\n",
      "Rewardworld:\n",
      "\n",
      "[['O' 'O' 'O' 'O' 'O' 'X' '-' 'O' 'O' 'O']\n",
      " ['O' 'O' '-' 'X' 'O' 'O' 'O' 'O' 'O' 'O']\n",
      " ['ยง' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'X']\n",
      " ['X' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' 'O' 'O' 'O' 'X' 'X' 'O' 'O']\n",
      " ['O' 'X' 'O' '-' 'O' 'O' 'O' 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' 'O' 'O' 'O' 'X' 'O' 'X' 'O']\n",
      " ['O' 'O' 'X' 'O' '-' '-' 'O' 'O' 'O' 'O']\n",
      " ['X' '+' 'O' 'O' 'O' 'O' 'X' 'O' 'ยง' 'O']\n",
      " ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O']]\n",
      "\n",
      "\n",
      "Learned moves: \n",
      "\n",
      "[['right' 'down' 'down' 'left' 'left' 'None' 'right' 'left' 'left' 'None']\n",
      " ['right' 'left' 'left' 'None' 'right' 'left' 'left' 'up' 'left' 'left']\n",
      " ['right' 'up' 'left' 'down' 'up' 'down' 'up' 'left' 'left' 'None']\n",
      " ['None' 'right' 'left' 'right' 'left' 'left' 'left' 'left' 'left' 'left']\n",
      " ['down' 'left' 'up' 'down' 'up' 'left' 'None' 'None' 'right' 'left']\n",
      " ['right' 'None' 'right' 'left' 'left' 'up' 'up' 'right' 'left' 'left']\n",
      " ['left' 'left' 'None' 'None' 'right' 'left' 'None' 'left' 'None' 'right']\n",
      " ['right' 'left' 'None' 'None' 'right' 'left' 'left' 'left' 'right'\n",
      "  'left']\n",
      " ['None' 'down' 'None' 'None' 'None' 'left' 'None' 'None' 'up' 'left']\n",
      " ['up' 'left' 'None' 'None' 'None' 'None' 'None' 'right' 'up' 'left']]\n"
     ]
    }
   ],
   "source": [
    "# print learned policy for every state\n",
    "\n",
    "moves_ls = []\n",
    "dummy = np.array([0.,0.,0.,0.])\n",
    "\n",
    "for i in Q:\n",
    "    if (i==dummy).all(): # if state only has value of 0s\n",
    "        moves_ls.append(\"None\")\n",
    "        \n",
    "    else:\n",
    "        moves_ls.append(dict_moves[np.argmax(i)])\n",
    "    \n",
    "moves_ls = np.array(moves_ls)\n",
    "moves_ls = moves_ls.reshape(dim,dim)\n",
    "\n",
    "gw.reset() \n",
    "gw.visualize()\n",
    "\n",
    "print(\"Learned moves: \\n\")\n",
    "print(moves_ls)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": " Sarsa.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
